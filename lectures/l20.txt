6.033 2012 Lecture 20: Replicated state machines

Topics:
  Replicated state machines.
  Paxos.

Administrivia.
  Quiz 2 solutions posted.
  [ slide: 6.033 for Big Screw ]

Fault-tolerance.
  Previous lecture: optimistic replication, tolerated inconsistency.
  This lecture: pessimistic replication, prevent inconsistency.

Why is pessimistic replication necessary?
  Some applications may prefer not to tolerate inconsistency.
    E.g., a replicated lock server, or replicated coordinator for 2PC.
    Better not give out the same lock twice.
    Better have a consistent decision about whether transaction commits.
  Trade-off: stronger consistency with pessimistic replication means
    lower availability than what you might get with optimistic replication.

Recall, problem with optimistic replication: replicas get out of sync.
  One replica writes data, another replica doesn't see the changes.
  This behavior was impossible with a single server.

Ideal goal for replicated system: single-copy consistency.
  Property of the externally-visible behavior of a replicated system.
  Operations appear to execute as if there's only a single copy of the data.
    Internally, there may be failures or disagreement, which we have to mask.
  Similar to how we defined serializability goal ("as if executed serially").

Replicating a server (e.g., a file server).
  [ diagram: two clients, two servers ]
  Strawman: clients send requests to both servers.
  Tolerating faults: if one server is down, clients send to the other.

  Tricky case: what if there's a network partition?
    [ diagram: client 1 reaches server 1, client 2 reaches server 2 ]
    Each client thinks the other server is dead, keeps using its server.
    Bad situation: not single-copy consistency!

Handling network partitions.
  Issue: clients may disagree about what servers are up.
  Hard to solve with just 2 servers, but possible with 3 servers.
  Idea: require a majority (strictly over half) servers to perform operation.
    In case of 3 servers, 2 form a majority.
    If client can contact 2 servers, it can perform operation (otherwise, wait).
    Thus, can handle any 1 server failure.
  Why does the majority rule work?
    Important property: any two majority sets of servers must overlap.
    Suppose two clients issue operations to a majority of servers.
    Must have overlapped in at least one server, will help ensure single-copy.

[ demo: replicating a server that tracks bank accounts ]
  Two terminals, one on top of another, both with rxvt-font.sh xft:Monospace-18

  top% cd rsm
  top% less srv.py
  top% ./srv.py /tmp/a 5001

  bottom% less client.py
  bottom% ./client.py 5001
  ^C
  bottom% cat /tmp/a

  ^C
  top% rm /tmp/a
  top% ./srv.py /tmp/a 5001 &
  top% ./srv.py /tmp/b 5002 &

  bottom% ./client.py 5001 5002
  ^C
  bottom% cat /tmp/a
  bottom% cat /tmp/b

  ## multiple clients: what goes wrong?
  bottom% ./client.py 5001 5002 &
  bottom% ./client.py 5001 5002
  ^S when first error appears
  ^Q to resume
  ^C

  bottom% fg
  ^C

Problem: replicas can become inconsistent.
  Issue: clients' requests to different servers can arrive in different order.
  How do we ensure the servers remain consistent?

Replicated state machines.
  A general approach to making consistent replicas of a server:
  - Start with the same initial state on each server.
  - Provide each replica with the same input operations, in the same order.
  - Ensure all operations are deterministic.
    E.g., no randomness, no reading of current time, etc.
  These rules ensure each server will end up in the same final state.

Simple implementation of RSM: replicated logs.
  [ diagram: each server has a log consisting of Wx, Ry ]
  Log contains client operations, including both writes and reads.
  Log entries are numbered.

Key issue: agreeing on the order of operations.
  [ diagram: coordinator receives operations from clients, sends to replicas ]
  Coordinator handles one client operation at a time.
  Coordinator chooses an order for all operations (assigns log sequence number).
  Coordinator issues the operation to each replica.
  When is it OK to reply to client?
    Must wait for majority of replicas to reply.
    Otherwise, if a minority crashes, remaining servers may continue without op.

[ demo: coordinator ]
  top% kill %1
  top% kill %2
  top% rm /tmp/a /tmp/b
  top% ./srv.py /tmp/a 5001 &
  top% ./srv.py /tmp/b 5002 &
  top% ./coord.py 5099 5001 5002

  bottom% ./client.py 5099 &
  bottom% ./client.py 5099 &
  bottom% jobs

  bottom% kill %1
  bottom% kill %2

Replicating the coordinator.
  Tricky: can we get multiple coordinators due to network partition?
  Tricky: what happens if coordinator crashes midway through an operation?

Paxos: fault-tolerant consensus.
  Set of machines can agree on one value.
    E.g., X is the next operation that everyone will execute.
    E.g., Y is the next coordinator.
  Works despite node failures, network failures, delays.

  Correctness:
    All nodes agree on the same value.
    The agreed-upon value was proposed by some node.
  Fault-tolerance:
    If less than N/2 nodes fail, rest should reach agreement w.h.p.
    Liveness not guaranteed (may take an arbitrarily long time).
  Assumes fail-stop machines.

Each machine runs three logical functions in Paxos:
  Proposer: tries to convince acceptors to agree on some value
    (e.g., an operation it just received from a client).
  Acceptor: persistently tracks proposals, vote on proposals.
  Learner: does something with the value, once agreement is reached
    (e.g., just the local RSM replica, which executes the chosen operation).

Using Paxos to reach consensus on operation order.
  Consider a single log entry (e.g., log entry #5).
  OK for any given replica not to know what operation is #5 yet.
  But all replicas that know about #5 must agree on what it is.
  Plan: run a separate instance of Paxos to agree on each entry (including #5).

Intuition for why fault-tolerant consensus difficult.
  Strawman 1: proposers agree on a single acceptor they will propose to.
    Acceptor approves the first proposal it receives, and rejects all others.
    Correct because only a single acceptor.
    Problem: not fault-tolerant!
  Strawman 2: proposers send proposal to all acceptors.
    Each acceptor approves the first proposal and rejects all others.
    Proposers declare victory after approval from majority of acceptors.
    Correct because there's only one majority, so at most one winner.
    Problem: noone might get a majority, if we have 3 proposers!
    Problem: proposer might get majority and then crash!

Paxos: two phases for consensus.
  Prepare: agree on a proposal number (value might not be decided yet).
  Accept: agree on a value, for given proposal number.

  Think of proposal numbers as different attempts to reach consensus.
  If one attempt fails (e.g., 3 proposers can't get majority),
    try again with a larger proposal number.
  Key requirement: if an earlier proposal number accepted a value,
    have to make sure later proposals accept the same value.

Paxos state maintained by acceptor (persistent across reboots):
  Np: largest proposal number seen in Prepare.
  Na: largest proposal number seen in Accept.
  Va: value accepted for proposal Na.

[ slide: Paxos ]

Example 0: Paxos accepting value 'x' with one proposer.
  A1: Prep(10)->OK,None  Acc(10, x)->OK
  A2: Prep(10)->OK,None  Acc(10, x)->OK
  A3: Prep(10)->OK,None  Acc(10, x)->OK

  Note: node ID is part of proposal number, for uniqueness.

What is the commit point -- i.e., when is the value 'x' chosen?
  Majority of acceptors write the corresponding Na / Va to disk.

What if the Accept message to one of the acceptors is lost?
  Before majority of acceptors wrote to disk -> no consensus yet, fine.
  After consensus -> proposer can start a new round, will pick same Va.
  Why?  At least one Prepare_OK reply will include that Va.

Example 1: Paxos with two proposers, choosing between 'x' and 'y'.
  One proposer has N=10, another has N=11 (e.g., low bits are the node ID).

  A1: Prep(10)  Prep(11)  Acc(10, x)  Acc(11, y)
  A2: Prep(10)  Prep(11)  Acc(10, x)  Acc(11, y)
  A3: Prep(10)  Prep(11)  Acc(10, x)  Acc(11, y)

  Both proposals were OKed by Prepare; can Paxos accept x and then y?
  Steps:
    First proposer issues Prepare, gets Prepare_OK.
    Second proposer issues Prepare, gets Prepare_OK.
    Second proposer sends out Accept, gets consensus.
    First proposer sends out Accept, but gets ignored (proposal number is low)!
  Key point: Accept() checks if it heard about a newer proposal.
    If it has, then the older proposal number loses: Accept(10, x) will reject.
    If it hasn't, will remember the accepted value: e.g., Prepare(11) is later.
    Then, any newer Prepare() will learn about x.

Example 2: Paxos with proposer crashing after majority of Accepts are processed.
  A1: Prep(10)  Acc(10, x)  Prep(11)  Acc(11, x)
  A2: Prep(10)  Acc(10, x)  Prep(11)  Acc(11, x)
  A3: Prep(10)              Prep(11)  Acc(11, x)

  Second proposer tries to agree on a new value y.
  Gets Prepare_OK with first proposer's value.
  Re-sends Accept messages with first proposer's value.

  Key point: proposer conservatively uses a previous value from Prepare_OK().
    Even if just one acceptor got it: could be the 1 overlap between majorities.

What happens if acceptor fails after accepting, before sending Accept_OK?
  Without a majority of acceptors left, must wait for acceptor to recover.
  Acceptors must record Na/Va persistently on disk.

What happens if acceptor fails after sending Prepare_OK?
  Also need to remember Np persistently on disk.
  Otherwise, example 1 fails if all acceptors reboot: should not accept (10, x).

In practice, Paxos typically not used to agree on every single operation.
  Instead, Paxos used to agree on who is the new coordinator (+ some extras).
  Coordinator can efficiently order requests (e.g., our simple RSM design).
  When coordinator seems to be down, use Paxos to agree on new state.
  Must also use Paxos to agree to some extra information:
    - Set of all replicas, for when we need to elect the next coordinator.
    - Last operation executed by the current (dead) coordinator, so we
      don't lose track of it during switchover.
  Can also add replicas at runtime: just agree on new replica set.
    Need a way to transfer log state to the new replica.

[ slide: summary ]

